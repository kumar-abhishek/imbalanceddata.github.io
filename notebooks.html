    <h2>Python notebooks</h2>

    <ul>
        <li> Notebook 1.1 - imbalanced-learn library demo: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter01">ipynb/colab</a></li>
        
        <li> Notebook 2.1 - Oversampling techniques(RandomOverSampler, SMOTE, Borderline-SMOTE, ADASYN): <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter02/chapter02.ipynb">ipynb/colab</a> </li>
        <li> Notebook 2.2 - Performance comparison of oversampling techniques on various datasets: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter02/model-perf-comparison.ipynb">ipynb/colab</a> </li>
        <li> Notebook 2.3 - Highlighting problems with SMOTE: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter02/smote-plots.ipynb">ipynb/colab</a> </li>
        
        <li> Notebook 3.1 - RandomUnderSampler, ClusterCentroids, RENN, AllKNN, Tomek links, Neighborhood cleaning rule, Instance hardness, Condensed nearest neighbors, One sided selection: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter03/chapter03.ipynb">ipynb/colab </a>
        </li>
        <li> Notebook 3.2 - Performance comparison of undersampling techniques on various datasets: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter03/model_perf_comparison_undersampling.ipynb">ipynb/colab </a>
        </li>
        <li> Notebook 4.1 - Random Forest, Balanced Random Forest with undersampling, BaggingClassifier, Underbagging, Overbagging,  SMOTEBagging, AdaBoostClassifier, RUSBoostClassifier, EasyEnsemble, XGBoost: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter04/chapter04.ipynb">ipynb/colab </a>
        </li>
        <li> Notebook 4.2 - Performance comparison of ensembling methods on various datasets: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter04/model_perf_comparison_ensembling.ipynb">ipynb/colab </a>
        </li>
   
        
        <li> Notebook 5.1 - Using class_weight with various sklearn/XGBoost models, Metacost: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter05/chapter05.ipynb">ipynb/colab </a>
        </li>
        <li> Notebook 5.2 - Threshold tuning using ROC curve, PR curve, custom metric: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter05/threshold_computation.ipynb">ipynb/colab </a>
        </li>
    
        
        <li> Notebook 6.1 - Simple neural network trainin: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter06/chapter06.ipynb">ipynb/colab </a>
        </li>
        <li> Notebook 6.2 - Multi-class classification: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter06/multiclass_classification_PR_curve.ipynb">ipynb/colab </a>
        </li>

        <li> Notebook 7.1 - Implementing Augmix with FashionMNIST dataset: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter07/Augmix_FashionMNIST.ipynb">ipynb/colab </a>
        </li>
        
        <li> Notebook 7.1 - Implementing Augmix with FashionMNIST dataset: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter07/Augmix_FashionMNIST.ipynb">ipynb/colab </a></li>
        <li> Notebook 7.2 - Implementing Cutmix, Mixup, and Remix techniques on FashionMNIST dataset: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter07/Cutmix_mixup_remix_FashionMNIST.ipynb">ipynb/colab </a></li>
        <li> Notebook 7.3 - Data-level techniques for Natural Language Processing: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter07/Data_level_techniques_NLP.ipynb">ipynb/colab </a></li>
        <li> Notebook 7.4 - Implementing dynamic sampling techniques: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter07/Dynamic_sampler.ipynb">ipynb/colab </a></li>
        <li> Notebook 7.5 - Variational Autoencoder (VAE) with MNIST dataset: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter07/VAE_MNIST.ipynb">ipynb/colab </a></li>
        <li> Notebook 7.6 - Implementing Cutmix technique: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter07/cutmix.ipynb">ipynb/colab </a></li>
        <li> Notebook 7.7 - Implementing Cutout technique: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter07/cutout.ipynb">ipynb/colab </a></li>
        <li> Notebook 7.8 - Implementing Mixup technique: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter07/mixup.ipynb">ipynb/colab </a></li>
        <li> Notebook 7.9 - Plotting data transformations: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter07/plot_transforms.ipynb">ipynb/colab </a></li>
 
       
        <li> Notebook 8.1 - Focal Loss with CIFAR10 Long-Tailed Dataset: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter08/CIFAR10_LT_Focal_Loss.ipynb">ipynb/colab </a></li>
        <li> Notebook 8.2 - Implementing Class-Dependent Temperature (CDT) Loss: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter08/Class_Dependent_Temperature_(CDT)_Loss.ipynb">ipynb/colab </a></li>
        <li> Notebook 8.3 - Exploring Class Balanced Loss: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter08/Class_balanced_loss.ipynb">ipynb/colab </a></li>
        <li> Notebook 8.4 - Class-wise Difficulty Balanced Loss: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter08/Class_wise_difficulty_balanced_loss.ipynb">ipynb/colab </a></li>
        <li> Notebook 8.5 - Deferred Reweighting (DRW) Technique: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter08/Deferred_reweighting_DRW.ipynb">ipynb/colab </a></li>
        <li> Notebook 8.6 - Tweet Emotion Detection Using Class Weights with Huggingface: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter08/Tweet_emotion_detection_using_class_weights_Huggingface.ipynb">ipynb/colab </a></li>
        <li> Notebook 8.7 - Class Weighting in PyTorch for Imbalanced Datasets: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter08/class_weighting_pytorch_imbalanced_dataset.ipynb">ipynb/colab </a></li>

        
        <li> Notebook 9.1 - Graph Neural Network: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter09/GNNs.ipynb">ipynb/colab </a>
        </li>
        <li> Notebook 9.2 - Online hard example mining (OHEM): <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter09/Online_hard_example_mining-OHEM.ipynb">ipynb/colab </a>
        </li>
        <li> Notebook 9.3 - Class rectification loss: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter09/Class_Rectification_Loss.ipynb">ipynb/colab </a>
        </li>
        
        <li> Notebook 10.1 - Reliability diagram, expected calibration error(ECE), maximum calibration error(MCE) : <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter10/calibration_MNIST.ipynb">ipynb/colab </a>
        </li>
        
        <li> Notebook 10.2 - Impact of oversampling/undersampling/class-weighting techniques on calibration of a model: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter10/calibration_to_account_for_sampling_or_weighting.ipynb">ipynb/colab </a>
        </li>
        
        <li> Notebook 10.3 - Impact of oversampling/undersampling/class-weighting techniques on calibration of a model: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter10/model-calibration.ipynb">ipynb/colab </a>
        </li>
        
        <li> Notebook 10.4 - Model calibration on Kaggle HR data: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter10/model_calibration_on_Kaggle_HR_data.ipynb">ipynb/colab </a>
        </li>
        <li> Notebook 10.5 - Plat's scaling and isotonic regression: <a
                href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/chapter10/platts_scaling_and_isotonic_regression.ipynb">ipynb/colab </a>
        </li>
    </ul>


    <br>
    <h2>Citation</h2>
    <pre><code>
 @book{kumarabhishek2023dataimbalance,
 author = "Kumar Abhishek",
 title = "Machine Learning for Imbalanced Data",
 publisher = "Packt",
 year = 2023,
 url = "https://imbalanceddata.com"
}
    </code></pre>
